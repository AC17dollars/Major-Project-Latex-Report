\section{\MakeUppercase{Literature Review}}
Video compression, a critical technology in the digital age, has evolved significantly over the years to meet the increasing demand for efficient data storage and transmission. Initially, human-devised algorithms and methods laid the foundation for video compression techniques. As technology advanced, a myriad of sophisticated algorithms and methodologies emerged, enabling more effective and efficient compression. From traditional methods like \gls{dct} and motion estimation to cutting-edge neural network-based approaches, the field of video compression boasts a diverse array of solutions. Numerous systems and tools have been developed to compress and decompress video content autonomously. This literature review summarizes some of the key methodologies that have been developed to address the challenges of video compression, highlighting the evolution from traditional techniques to modern machine learning-based solutions.

The MPEG-1 Audio standard \cite{mp3_1994}, a component of the broader MPEG multimedia format, was developed to enhance the efficiency of digital audio compression. This standard comprises three distinct layers of compression—Layer 1, Layer 2, and Layer 3—with increasing complexity and efficiency. Layer 1 offers simplicity in implementation, while Layer 3, also known as MP3, provides superior performance at lower bit rates through sophisticated techniques such as Huffman coding and psychoacoustic models. These models are pivotal in predicting human auditory responses and minimizing perceptible noise caused by the compression process. By adapting the characteristics of the compression dynamically, the standard aims to maintain audio fidelity even at high compression ratios, thus facilitating efficient digital broadcasting and storage without compromising quality

The H.264 video coding standard \cite{h264_2006}, developed collaboratively by the Video Coding Expert Group and the Moving Pictures Expert Group, represents a significant advance in video compression technology. Officially known as MPEG-4 Part 10, \gls{avc}, this standard offers enhanced coding efficiency with a simpler syntax compared to its predecessors, making it highly compatible across various network protocols and multiplex architectures. Notably, H.264 incorporates features like intrapicture prediction, multiple reference pictures, variable block sizes, and an in-loop deblocking filter, which collectively contribute to its superior performance in applications ranging from video conferencing to video streaming and broadcasting across different transmission mediums

The \gls{hevc} standard \cite{h265_2012}, also known as H.265, has emerged as a successor to the widely used H.264/AVC, setting new benchmarks in video compression technology. Introduced by the Joint Collaborative Team on Video Coding, \gls{hevc} primarily aims to address the increasing demand for higher video resolutions and quality, such as ultra-high definition and 4K, particularly over bandwidth-constrained networks. By incorporating advancements such as improved block partitioning flexibility, enhanced prediction capabilities, and increased parallel processing support, \gls{hevc} effectively doubles the compression efficiency compared to its predecessor. This allows \gls{hevc} to deliver similar video quality at approximately half the bitrate of H.264/AVC, which significantly optimizes data usage and transmission costs, particularly important for streaming high-definition video in mobile environments and over the internet.

The paper ``Deep Contextual Video Compression" \cite{li2021deep} introduces a conditional coding framework that utilizes deep learning to encode videos by using contextual information rather than just the residues. Deep contextual video compression framework advances this concept by using feature domain context as a condition, enabling the encoder and decoder to utilize high-dimensional information for reconstructing high-frequency content and achieving superior video quality. This approach addresses the limitations of predictive coding and also offers extensibility for tailored conditions, significantly outperforming previous state-of-the-art methods with a 26.0\% bitrate saving for 1080P standard test videos compared to the x265 \gls{codec} using the very slow preset.

Further deepening our understanding, the exploration of latent spaces in ``Deep Learning in Latent Space for Video Prediction and Compression" \cite{Liu_latentvideo} reveals that employing deep autoencoders trained with \gls{gan}s can effectively compress video data. This novel framework adopts a \gls{gan}-based autoencoder architecture, accompanied by trainable quantization and entropy coding, to compress video sequences within a latent vector space. This method begins by learning efficient latent space representations of video frames via a deep autoencoder trained with \gls{gan}s, followed by inter-frame prediction using a convolutional \gls{lstm} network. This approach only stores the differences between predicted and actual representations in a compact latent space, reducing residual entropy. Additionally, the framework demonstrates its versatility by applying the same Conv\gls{lstm} predictor to both video compression and anomaly detection tasks. Performance evaluations show that this method achieves superior or competitive results compared to other state-of-the-art learning-based \gls{codec}, offering a wide range of rate-distortion trade-offs and effectively reducing both spatial and temporal redundancies.

Adding to the foundational concepts, the paper ``NeRV: Neural Representations for Videos" \cite{chen2021nerv} provides the underlying methodology that has inspired subsequent developments in neural-based video compression. In this paper, the authors propose \gls{nerv}, a novel neural representation for videos, which encodes videos into neural networks, treating videos as neural networks and converting video encoding to model fitting and decoding as a simple feed forward operation. Unlike conventional representations that treat videos as frame sequences, \gls{nerv} represents videos as neural networks, taking frame index as input and outputting the corresponding \gls{rgb} image. This representation improves encoding speed by 25x to 70x, decoding speed by 38x to 132x, while achieving better video quality compared to pixel-wise implicit representations. \gls{nerv} allows for the conversion of video compression problems to model compression problems, enabling the use of standard model compression tools and achieving comparable performance with conventional video compression methods such as H.264 and \gls{hevc}. \gls{nerv} shows promising results in other tasks such as video denoising, outperforming traditional hand-crafted denoising algorithms and ConvNets-based methods.

In ``Effective and Efficient Video Compression by the Deep Learning Techniques," \cite{paneerselvam_effective} a novel video compression method using deep learning frameworks, specifically combining \gls{cnn} and \gls{gan} is proposed, with an efficient workflow for frame-level compression. The method involves dividing video frames into different groups, using CNN to eliminate duplicate frames, and replacing them with single images by detecting minor changes through \gls{gan}, which are then recorded using \gls{lstm}. Instead of the entire image, only the small changes detected by GAN are substituted, significantly enhancing compression efficiency. This is complemented by pixel-wise comparisons using \gls{knn}, clustering via K-means, and applying \gls{svd} on each frame across the \gls{rgb} color channels to reduce the utility matrix's dimensions by extracting latent factors. The processed video frames are then encoded into video format, and experiments demonstrated substantial size reductions with minimal quality loss, achieving around a 10\% deviation in quality and more than a 50\% reduction in size compared to the original video. The method, which also includes segmenting videos into key frames and leveraging deep learning networks for object detection, showed superior compression ratios and rate-distortion cost compared to existing \gls{vvc} methods

Another significant stride is presented in ``Video Compression with Entropy-constrained Neural Representations," \cite{Gomes_2023_CVPR} where the compression occurs not in pixel space but within the neural network parameter space. According to the paper, encoding videos as neural networks (known as \gls{nvr}) offers innovative video processing capabilities, though traditional methods still outperform \gls{nvr}s in video compression due to inefficiencies in compact spatio-temporal representation and disjoint rate-distortion optimization. To address these issues, they propose a novel convolutional architecture that effectively represents spatio-temporal information and employs a joint optimization strategy for rate and distortion. This end-to-end approach eliminates the need for post-training operations like quantization or pruning. Evaluations on the Ultra Video Group dataset show that their method achieves state-of-the-art results, surpassing both traditional \gls{hevc} benchmarks and established neural video compression methods. 

The paper ``Neural Video Compression with Diverse Contexts" \cite{li2023neural} proposes enhancing context diversity in both temporal and spatial dimensions. Temporally, the model is guided to learn hierarchical quality patterns across frames, enriching long-term, high-quality temporal contexts. Additionally, group-based offset diversity is introduced to improve temporal context mining within an optical flow-based coding framework. Spatially, a quadtree-based partitioning method increases spatial context diversity for more effective entropy coding.To tap the potential of optical flow-based coding framework, a group-based offset diversity where the cross-group interaction for better context mining is proposed. Experiments demonstrate that this \gls{codec} achieves a 23.5\% bitrate reduction over previous state-of-the-art \gls{nvc}s and surpasses the developing next-generation traditional \gls{codec} \gls{ecm} in both \gls{rgb} and YUV420 colorspaces in terms of \gls{psnr}. 

While \gls{inr}s have been successfully applied in image and 3D shape compression, their potential for audio compression remains largely unexplored. In the study ``Siamese SIREN: Audio Compression with Implicit Neural Representations" \cite{lanzendörfer2023siamese}, the authors introduce Siamese \gls{siren}, an \gls{inr} model built on the \gls{siren} architecture, which aims to achieve superior audio reconstruction fidelity with fewer network parameters compared to previous architectures. Siamese \gls{siren} utilizes two twin extensions to the standard \gls{siren} model to approximate the original audio signal, using the difference in noise between the two extensions for noise removal in the reconstruction process. The study evaluates the trade-off between compression speed, quality, and ratio, finding that longer training times can reduce noise presence but still leave it audibly detectable. To address this, the authors propose an approach for estimating and removing noise, achieving promising results in audio compression. The proposed Siamese \gls{siren} network reduces parameter count by merging a subset of layers, allowing for effective noise removal without doubling the parameter count.

``Boosting Neural Representations for Videos with a Conditional Decoder" \cite{zhang2024boosting} further explores the efficiency of neural networks in video compression by utilizing the \gls{nerv} technique, which represents each video frame as weights of a neural network. In this paper, the authors propose a universal boosting framework for implicit video representations, aiming to enhance representation capabilities and accelerate convergence speed. Their framework includes a conditional decoder with a temporal-aware affine transform module, a sinusoidal \gls{nerv}-like block for generating diverse intermediate features, and a high-frequency information-preserving reconstruction loss. They introduce a consistent entropy minimization scheme and develop video \gls{codec} based on the boosted representations. Experimental results on the Ultra Video Group dataset demonstrate significant improvements over baseline methods in tasks such as video regression, compression, inpainting, and interpolation. The authors validate the contribution of each component through extensive ablation studies, setting a new benchmark in the field of implicit video representation.

The paper ``Implicit Neural Representations with Periodic Activation Functions" \cite{sitzmann2020implicit} introduces Sinusoidal Representation Networks (\gls{siren}) which utilize sinusoidal (sine) activation functions within the framework of \gls{inr} to model discrete audio and video signals, leveraging the continuous and differentiable nature of sine functions for high-fidelity representations. For audio, \gls{siren}s map temporal coordinates to amplitude values of waveforms, capturing cyclical patterns in audio data and allowing for continuous modeling over time, which is beneficial for generating synthetic speech or music. In video, \gls{siren}s treat signals as functions of spatial and temporal coordinates, using sine functions to manage complexities such as rapid pixel intensity changes and motion, thus enabling continuous and differentiable representation that captures dynamics like motion blur and lighting variations. This continuous representation enhances the processing of derivatives, aiding in video tasks like motion estimation and compression, where detailed frame-by-frame analysis is essential.

While implicit neural representations \gls{inr} have garnered significant attention for their capability to model complex signals like videos and audio through neural networks. Recent advancements have predominantly focused on video \gls{inr}s, such as \gls{siren} and \gls{nerv}, which use spatial and temporal coordinates or frame indices to efficiently generate video frames. However, audio representation via \gls{inr} remains underexplored, with most existing methods relying solely on \gls{mlp}s, leading to limitations in parameter efficiency and model expressibility. In response, \gls{nera} and \gls{nerva} frameworks \cite{NeRVA-2024} introduce novel approaches for timestamp-based audio and joint video-audio representations. \gls{nera} employs a hybrid architecture combining \gls{mlp}s and convolutional blocks, optimized for audio-specific characteristics. \gls{nerva} extends this concept by enabling unified representation of multimedia content, utilizing a multi-branch architecture for audio and video modalities.

Quantization in deep learning, as detailed in the PyTorch documentation \cite{pytorchQuantizationx2014}, is a technique that reduces the precision of numerical representations, such as using 8-bit integers instead of 32-bit floating-point numbers for weights and activations. This approach significantly improves model efficiency, reducing computation and storage requirements, particularly for resource-constrained environments like edge devices. PyTorch supports various quantization techniques, including static quantization, which precomputes scaling factors, dynamic quantization, applied at runtime for weight optimization, and quantization-aware training (QAT), which adapts models to lower precision during training. These methods help minimize the trade-off between reduced precision and accuracy loss. With user-friendly APIs, PyTorch simplifies quantization workflows for post-training or integration during model development. By enabling hardware compatibility and improved inference performance, quantization serves as a vital tool for deploying deep learning models efficiently in real-world scenarios.

The KD-INR framework \cite{DBLP:journals/tvcg/HanZB24} addresses the challenge of compressing large-scale time-varying data by combining knowledge distillation and implicit neural representations. Unlike traditional methods that require the entire dataset during training, KD-INR enables sequential learning of each time step through a two-stage process: spatial compression and model aggregation. Spatial compression uses bottleneck layers and features of interest preservation-based sampling to reduce data size while retaining crucial information, and model aggregation employs an offline knowledge distillation algorithm to merge multiple models into a single compact one. Evaluated against state-of-the-art methods like CoordNet, Neur Comp, SIREN, SZ3, ZFP, and TTHRESH, KD-INR demonstrates superior performance in terms of PSNR, LPIPS, and visual quality. Its ability to independently compress each time step makes it ideal for real-time applications such as in-place simulations, allowing for efficient compression and high-quality decompression during post-hoc analysis.

While significant progress has been made in the field of video and audio compression using implicit neural representations (\gls{inr}s), existing methodologies have primarily treated video and audio as separate entities. Techniques like \gls{siren} and Siamese \gls{siren} have demonstrated remarkable potential for high-fidelity representations within their respective domains. However, a unified approach that simultaneously represents and compresses both video and audio signals within a single \gls{inr} framework remains largely unexplored. Recent research, such as \gls{nerva}, has successfully represented both audio and video within the same model framework. However, these methods adopt a frame-wise approach, combining \gls{mlp}s and convolutional blocks for representation. In contrast, representing both video and audio on a pixel-wise basis using only \gls{mlp}s has not yet been attempted. This project aims to address this gap by developing a comprehensive \gls{inr} model capable of jointly representing and compressing multimedia content at a pixel-wise level using \gls{mlp}s alone. Advanced compression techniques such as quantization, efficient encoding strategies, and knowledge distillation will be integrated. Knowledge distillation will enable the transfer of knowledge from a larger, more complex model to a smaller, optimized model, ensuring efficient compression without sacrificing representation quality. These enhancements are expected to improve compression efficiency, reduce computational and storage requirements, and maintain high fidelity. This unified approach offers a transformative solution for processing and analyzing multimedia content in a resource-efficient manner.
\pagebreak
